# Maximum likelihood estimation

::: {.callout-note}

## Preparation

Download influenza incidence data here:

{{< downloadthis data/flu_inc.csv dname="flu_inc" label="flu_inc.csv" type=light >}}

:::

## Likelihood

$$\mathbb{L}(\theta | x) = \mathbb{P}(x | \theta)$$

## Methods

The maximum likelihood estimation follows 4 steps:

- **Step 1**. Write down the likelihood function $L(\theta)$. Likelihood function is a function of $\theta$, which is the product of the $n$ mass/density function terms of the observed values $y_i$.

$$\mathbb{L}(\theta) = \prod_{i = 1}^{n} f_Y(y_i| \theta)$$

- **Step 2**. Take the natural log of the likelihood $log L(\theta)$, or log-likelihood. The production in likelihood now becomes the sum in log-likelihood. See @sec-calib-loglik for more details.

$$log \mathbb{L}(\theta) = \sum_{i = 1}^{n} log \left[ f_Y(y_i| \theta) \right]$$

- **Step 3**. Take the negative log-likelihood $- log L(\theta)$. Because optimizers work by minimizing a function, we have to work on negative log-likelihood. See @sec-calib-negloglik for more details.

$$- log \mathbb{L}(\theta) = - \sum_{i = 1}^{n} log \left[ f_Y(y_i| \theta) \right]$$

- **Step 4**. The optimizer try different values of $\theta$ in the paramater space $\Theta$ for which negative log-likelihood $- log \mathbb{L}(\theta)$ is minimized.

## 

## Code

```{r, message=FALSE, warning=FALSE}
library(tidyr)
library(odin)
library(bbmle)
library(ggplot2)
library(RColorBrewer)
```

We will use the same data of a H1N1 influenza outbreak in an elementary school in @sec-lse-code and fit the deterministic SEIR model with code @lst-seir-det.

```{r}
df <- read.csv("data/flu_inc.csv")
```

```{r, results=FALSE, message=FALSE, echo=FALSE}
odin_seir <- odin::odin({
  # Derivatives
  deriv(S) <- -beta * S * I
  deriv(E) <- beta * S * I - sigma * E
  deriv(I) <- sigma * E - gamma * I
  deriv(R) <- gamma * I
  deriv(CInc) <- sigma * E
  
  # Initial conditions
  initial(S) <- S_init
  initial(E) <- E_init
  initial(I) <- I_init
  initial(R) <- R_init
  initial(CInc) <- CInc_init
  
  # Parameters and initial values
  beta <- user(0.004)
  sigma <- user(0.5)
  gamma <- user(0.5)
  S_init <- user(369)
  E_init <- user(0)
  I_init <- user(1)
  R_init <- user(0)
  CInc_init <- user(0)
})

seir_mod <- function(beta, sigma, gamma, S0, E0, I0, R0, times) {
  # Set values for a new run
  odin_run <- odin_seir$new(beta = beta, sigma = sigma, gamma = gamma, S_init = S0, E_init = E0, I_init = I0, R_init = R0)
  
  # Run the model
  out <- data.frame(odin_run$run(times))

  # Compute incidence from cumulative incidence
  out$Inc <- c(I0, diff(out$CInc))
  out$CInc <- NULL
  out
}
```

Looking at the original paper, the school has 370 students [@cauchemez2011]. So we set the initial values as below.

```{r}
S0 <- 369
E0 <- 0
I0 <- 1
R0 <- 0
```

- **Step 1**. Write down the likelihood function $L(\theta)$.

We assume the incidence data is generated from a normal distribution with mean $\mu_{inc}$ is the predicted incidence and a standard deviation $\sigma_{inc}$.

$$L(\theta) = \prod_{i = 1}^{n} \mathcal{N}(\mu_{inc}, \sigma_{inc})$$

We use the `dnorm()` function to define this likelihood.

```{r, eval=FALSE}
dnorm(x = data, mean = pred, sd = sd_inc)
```

- **Step 2**. Take the natural log of the likelihood $log L(\theta)$, product becomes sum.

$$log L(\theta) = \sum_{i = 1}^{n} log \left[ \mathcal{N}(\mu_{inc}, \sigma_{inc}) \right]$$

```{r, eval=FALSE}
sum(dnorm(x = data, mean = pred, sd = sd_inc, log = T))
```

- **Step 3**. Take the negative log-likelihood $- log L(\theta)$.

$$- log L(\theta) = - \sum_{i = 1}^{n} log \left[ \mathcal{N}(\mu_{inc}, \sigma_{inc}) \right]$$

```{r, eval=FALSE}
- sum(dnorm(x = data, mean = pred, sd = sd_inc, log = T))
```

- **Step 4**. Pass the negative log-likelihood $- log L(\theta)$ to the optimizer.

[Root mean squared error as SD](https://hankstevens.github.io/Primer-of-Ecology/disease.html)

```{r}
mll <- function(beta, sigma, gamma, sd_inc) {
  # Make sure that parameters are positive
  beta <- exp(beta) 
  sigma <- exp(sigma)
  gamma <- exp(gamma)
  sd_inc <- exp(sd_inc)
  
  pred <- seir_mod(beta = beta, sigma = sigma, gamma = gamma, S0 = S0, E0 = E0, I0 = I0, R0 = R0, times = 0:(length(df$inc) - 1))
  pred <- pred$Inc
  # Return the negative log-likelihood
  - sum(dnorm(x = df$inc, mean = pred, sd = sd_inc, log = T))
}

starting_param_val <- list(beta = 0.004, sigma = 0.5, gamma = 0.5, sd_inc = 3)

estimates <- mle2(minuslogl = mll, start = lapply(starting_param_val, log), method = "Nelder-Mead")

summary(estimates)
```

```{r}
params <- exp(coef(estimates))
params
```

Compare the model with data.

```{r}
#| code-fold: true
#| fig-width: 4
#| fig-height: 2.5
#| out-width: "100%"
pred <- seir_mod(beta = params[1], sigma = params[2], gamma = params[3], S0 = S0, E0 = E0, I0 = I0, R0 = R0, times = df$day)
df_plot <- pred[,c("t", "Inc")]

# Compute 95% confidence interval
lwr <- qnorm(p = 0.025, mean = pred$Inc, sd = params[4])
upr <- qnorm(p = 0.975, mean = pred$Inc, sd = params[4])

my_palette <- brewer.pal(11, "PuOr")[c(10, 1, 4, 3, 8)]

ggplot(df_plot, aes(x = t, y = Inc)) +
  geom_line(color = my_palette[3], linewidth = 1.2) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), fill = my_palette[3], alpha = 0.15) +
  geom_point(data = df, aes(x = day, y = inc)) +
  coord_cartesian(ylim = c(0, max(upr))) +
  labs(x = "Day", y = "Incidence") +
  theme_minimal()
```

It looks very similar to the model we fitted with LSE at @fig-lse-seir.

Let compute $R_0$. For a "closed population" SEIR model, $R_0 = \frac{\beta}{\gamma} S_0$. Again, this is similar to the $R_0$ estimated from LSE at @lst-lse-r0. The reason is MLE is equal to LSE when we pick the normal distribution likelihood, see @sec-calib-mle-lse for more details.

```{r}
rnum0 <- params[1] * S0 / params[3]
rnum0
```


