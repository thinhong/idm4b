---
title: "Model fitting"
execute: 
  echo: false
  warning: false
---

```{r}
#| results: false
library(tidyverse)
library(odin)
library(bbmle)
library(purrr)

# The observed data points
df <- read.csv("data/flu_inc.csv")

# A model to fit
odin_seir <- odin::odin({
  # Derivatives
  deriv(S) <- -beta * S * I
  deriv(E) <- beta * S * I - sigma * E
  deriv(I) <- sigma * E - gamma * I
  deriv(R) <- gamma * I
  deriv(CInc) <- sigma * E
  
  # Initial conditions
  initial(S) <- S_init
  initial(E) <- E_init
  initial(I) <- I_init
  initial(R) <- R_init
  initial(CInc) <- CInc_init
  
  # Parameters and initial values
  beta <- user(0.004)
  sigma <- user(0.5)
  gamma <- user(0.5)
  S_init <- user(369)
  E_init <- user(0)
  I_init <- user(1)
  R_init <- user(0)
  CInc_init <- user(0)
})

seir_mod <- function(beta, sigma, gamma, S0, E0, I0, R0, times) {
  # Set values for a new run
  odin_run <- odin_seir$new(beta = beta, sigma = sigma, gamma = gamma, S_init = S0, E_init = E0, I_init = I0, R_init = R0)
  
  # Run the model
  out <- data.frame(odin_run$run(times))

  # Compute incidence from cumulative incidence
  out$Inc <- c(I0, diff(out$CInc))
  out$CInc <- NULL
  out
}

S0 <- 369
E0 <- 0
I0 <- 1
R0 <- 0

beta_vals <- seq(0.001, 0.01, 0.001)
sigma_vals <- seq(1, 3, 0.1)
gamma_vals <- seq(1, 3, 0.1)

param_grid <- expand.grid(
  beta  = beta_vals,
  sigma = sigma_vals,
  gamma = gamma_vals
)

# Run over each combination of (beta, sigma, gamma) and combine into a single data frame
results_df <- pmap_dfr(param_grid, function(beta, sigma, gamma) {
  mod_out <- seir_mod(
    beta  = beta,
    sigma = sigma,
    gamma = gamma,
    S0    = S0,
    E0    = E0,
    I0    = I0,
    R0    = R0,
    times = 0:(length(df$inc) - 1)
  )
  # Add columns for parameter tracking
  mod_out$beta  <- beta
  mod_out$sigma <- sigma
  mod_out$gamma <- gamma
  
  mod_out
})

mll <- function(beta, sigma, gamma, sd_inc) {
  # Make sure that parameters are positive
  beta <- exp(beta) 
  sigma <- exp(sigma)
  gamma <- exp(gamma)
  sd_inc <- exp(sd_inc)
  
  pred <- seir_mod(beta = beta, sigma = sigma, gamma = gamma, S0 = S0, E0 = E0, I0 = I0, R0 = R0, times = 0:(length(df$inc) - 1))
  pred <- pred$Inc
  # Return the negative log-likelihood
  - sum(dnorm(x = df$inc, mean = pred, sd = sd_inc, log = T))
}

starting_param_val <- list(beta = 0.004, sigma = 0.5, gamma = 0.5, sd_inc = 3)

estimates <- mle2(minuslogl = mll, start = lapply(starting_param_val, log), method = "Nelder-Mead")

params <- exp(coef(estimates))

pred <- seir_mod(beta = params[1], sigma = params[2], gamma = params[3], S0 = S0, E0 = E0, I0 = I0, R0 = R0, times = df$day)
df_plot <- pred[,c("t", "Inc")]
df_plot <- merge(df_plot, df, by.x = "t", by.y = "day")
colnames(df_plot) <- c("day", "pred", "inc")

# Compute 95% confidence interval
lwr <- qnorm(p = 0.025, mean = pred$Inc, sd = params[4])
upr <- qnorm(p = 0.975, mean = pred$Inc, sd = params[4])
```

Model fitting, or **calibration**, is the process of **adjusting** a model's **parameters** so that the **model's predictions** align **as closely as possible** with **real-world observations**.

In simple terms, you run the model with a given set of parameters and compare its simulated predictions with observed data points. Consider the example of an SEIR model with the following differential equations:

$$\frac{dS}{dt} = -\beta SI$$

$$\frac{dE}{dt} = \beta SI - \sigma E$$

$$\frac{dI}{dt} = \sigma E - \gamma I$$

$$\frac{dR}{dt} = \gamma I$$

Below you'll find an interactive plot. The dots represent observed data points, and the line shows the model's predicted incidence cases. You can adjust the values of $\beta$, $\sigma$ and $\gamma$ using the controls. Your task is to change these parameters and see how the line shifts. Keep tweaking them until the line closely follows the dots - this means the model's predictions align well with the observed data.

```{r}
ojs_define(pred = results_df, data = df)
```

```{ojs}
ojspred = transpose(pred)
ojsdata = transpose(data)

// Define sliders for beta, sigma, gamma
viewof betaVal = Inputs.range(
  [0.001, 0.01], 
  { label: "Beta", value: 0.001, step: 0.001 }
)

viewof sigmaVal = Inputs.range(
  [1, 3], 
  { label: "Sigma", value: 1, step: 0.1 }
)

viewof gammaVal = Inputs.range(
  [1, 3], 
  { label: "Gamma", value: 1, step: 0.1 }
)

// Filter the big data frame to match the chosen values exactly
filtered = ojspred.filter(d =>
  d.beta === betaVal &&
  d.sigma === sigmaVal &&
  d.gamma === gammaVal
)

// Plot incidence line and observed data points
Plot.plot({
  width: 600,
  height: 300,
  grid: true,
  marks: [
    // Lines for each compartment
    Plot.line(filtered, { x: "t", y: "Inc", stroke: "orange", strokeWidth: 2 }),
    // Points for observed incidence
    Plot.dot(ojsdata, {
      x: "day",
      y: "inc",
      fill: "black",
      r: 3,              // point size
      title: d => `Inc = ${d.inc}`
    })
  ],
  y: {domain: [0, 16]}
})
```

## Methods

Model fitting methods typically include [@rui2024]:

- Least squares estimation (LSE)
- Maximum likelihood estimation (MLE):
  - Optimisation: Nelder-Mead
  - Simulated annealing
  - MCMC
  - Newton Raphson
- Bayesian methods

### Least squares estimation {#sec-calib-lse}

```{r}
#| fig-width: 5
#| fig-height: 2.5
#| out-width: "100%"
ggplot() +
  # Observed points
  geom_point(data = df_plot, aes(x = day, y = inc)) +
  # Fitted incidence line
  geom_line(data = df_plot, aes(x = day, y = pred),
            color = "orange", linewidth = 1.2) +
  # Vertical lines (residuals)
  geom_segment(
    data = df_plot,
    aes(x = day, xend = day, y = inc, yend = pred),
    color = "blue"
  ) +
  coord_cartesian(ylim = c(0, max(upr)), xlim = c(0, 30)) +
  labs(x = "Day", y = "Incidence") +
  theme_minimal()
```

$$RSS = \sum_{i = 1}^{n}[y_i - f(x_i)]^2$$ {#eq-calib-rss}

Least squares estimation corresponds to maximum likelihood estimation when the noise is normally distributed with equal variances.

### Maximum likelihood estimation {#sec-calib-mle}

```{r}
#| fig-width: 5
#| fig-height: 2.5
#| out-width: "100%"

sd_inc <- params[4]
k <- 2  # how many SDs on either side of the mean to plot
scale_factor <- 4

# 1) Build the full pdf line data for each day
df_bell_line <- df_plot %>%
  rowwise() %>%
  do({
    day_i     <- .$day
    inc_pred  <- .$pred
    # Go e.g. Â±3 SD around the fitted mean
    inc_seq   <- seq(inc_pred - k*sd_inc, inc_pred + k*sd_inc, length.out = 200)
    dens      <- dnorm(inc_seq, mean = inc_pred, sd = sd_inc)
    
    # Offset the day coordinate by the pdf * scale_factor
    day_bump  <- day_i + dens * scale_factor
    
    # Return coordinates for geom_line
    data.frame(
      day  = day_i,         # to group by day if needed
      x    = day_bump,
      y    = inc_seq
    )
  }) %>%
  ungroup()

# 2) For each observed data point, compute a short horizontal offset to the pdf curve
#    at exactly inc = inc_obs.
df_segments <- df_plot %>%
  mutate(
    dens_obs = dnorm(inc, mean = pred, sd = sd_inc),
    x0 = day,
    y0 = inc,
    x1 = day + dens_obs * scale_factor,  # horizontal offset
    y1 = inc
  )

ggplot() +
  # Normal pdf lines (vertical bumps)
  geom_line(data = df_bell_line,
            aes(x = x, y = y, group = day),
            color = "blue", alpha = 0.1) +
  # 3) Horizontal segments from each black dot to its pdf line
  geom_segment(
    data = df_segments,
    aes(x = x0, y = y0, xend = x1, yend = y1),
    color = "blue"
  ) +
  # Observed points
  geom_point(data = df_plot, aes(x = day, y = inc)) +
  # Fitted incidence line
  geom_line(data = df_plot, aes(x = day, y = pred),
            color = "orange", linewidth = 1.2) +
  coord_cartesian(ylim = c(0, max(upr)), xlim = c(0, 30)) +
  labs(x = "Day", y = "Incidence") +
  theme_minimal()
```

A generalization of the least squares idea is the likelihood.

#### Likelihood

The likelihood is the probability of observing data $x$ given that our model is true[^2], $\mathbb{P}(x|M)$.

#### Log-likelihood {#sec-calib-loglik}

We have a lot of data points, and they are independent. The probability of observing all these data points at the same time is the production of these likelihood $\mathbb{P}(x_1|M) \times \mathbb{P}(x_2|M) \times \mathbb{P}(x_3|M)... = \prod\mathbb{P}(x|M)$.

Multiplying things together, you will end up losing precision if the numbers are too low. Here you are dealing with probability (a value \< 1), multiplying 100 probabilities you will end up with 1e-100.

But remember that $log(a \times b) = log(a) + log(b)$, and very convenient that $log(1^{-100} = -230.2585)$.

So $log(\mathbb{P}(x_1|M) \times \mathbb{P}(x_2|M) \times \mathbb{P}(x_3|M)...)$ $=$ $log(\mathbb{P}(x_1|M)) + log(\mathbb{P}(x_2|M)) + log(\mathbb{P}(x_3|M))...$ and it is so much easier to handle.

#### Negative log-likelihood {#sec-calib-negloglik}

Because statistical packages optimizers work by minimizing a function. Minimizing means **decrease the distance of two distributions** to its lowest, this is fairly easy because we get this when the distance close to 0 (just like the trick we do in hypothesis testing).

Minimizing negative log-likelihood (meaning that $-1 \times \text{log-likelihood}$) is equivalent to maximizing the log-likelihood, which is what we want to do (MLE: maximum likelihood estimation).

#### When MLE is equal to LSE {#sec-calib-mle-lse}

Assuming the data is generated from a normal distribution with mean $\mu$ and a standard deviation $\sigma$.

-   **Step 1**. Write down the likelihood function $L(\theta)$.

$$L(\theta) = \prod_{i = 1}^{n} \mathcal{N}(\mu_i, \sigma) = \prod_{i = 1}^{n} \frac{1}{\sigma\sqrt{2 \pi}} \text{exp} \left[ \frac{-1}{2 \sigma^2} (y_i - \mu_i)^2 \right]$$

$$L(\theta) = \frac{1}{(\sigma \sqrt{2 \pi})^n} \text{exp} \left[ \frac{-1}{2 \sigma^2} \sum_{i = 1}^n (y_i - \mu_i)^2 \right]$$

Since $\sigma$ and $\sqrt{2 \pi}$ are constant.

$$L(\theta) \propto \text{exp} \left[ - \sum_{i = 1}^n (y_i - \mu_i)^2 \right]$$

-   **Step 2**. Take the natural log of the likelihood $log L(\theta)$.

$$log L(\theta) \propto - \sum_{i = 1}^n (y_i - \mu_i)^2$$

-   **Step 3**. Take the negative log-likelihood $- log L(\theta)$.

$$- log L(\theta) \propto \sum_{i = 1}^n (y_i - \mu_i)^2$$

This looks exactly like the residual sum of squares at @eq-calib-rss.

-   **Step 4**. The optimizer **minimize negative log-likelihood** $- log L(\theta)$. It does the same thing as the LSE finds the optimal parameters by **minimizing the RSS**.

Nelder-Mead is a numerical algorithm to find the minimum (or maximum) of a multidimentional function. It's a direct search method: calculate the value of the function and compare it to other values. It does not use derivatives.

It use a ***simplex*** for a search. A simplex is a 




